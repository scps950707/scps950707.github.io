<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[OSDI-Memory Management]]></title>
    <url>%2F2018%2F05%2F13%2FOSDI-Memory-Management%2F</url>
    <content type="text"><![CDATA[¶Memory Management ¶tags: NCTU OSDI ¶Memory access in a program 123456int main()&#123; int a = 5; int b = a + 6; return 0;&#125; 左側的是code segment右側的是data segment Physical memory的方法 ¶Physical and virtual address ¶Physical Memory vs. Virtual Memory Why not physical memory? Benefits for using virtual memory Physical memory shared by multiple processes Isolate processes from each other Large/contiguous per process address space Use memory more efficiently ex:Pyscial memory執行時要將所有program片段讀到memory但是virtual memory不需要在branch很多的program較有優勢 Disadvantage for using virtual memory Expensive and slow What kind of memory will you use if we only have single process?A:關掉MMU or you are the only developer of all processes, or you need very fast memory accesses?A:Physical ¶Virtual addresses The memory addresses users and compiler believed The code/data are stored in physical memory The virtual memory addresses are translated into physical memory addresses (by CPU) and then CPU can access the code/data How can a CPU translate the addresses? Offset? Table lookup? instruction和data的virtual address本身都會透過CPU上的MMU去做轉換 ¶Segmentation 轉換的table可以放 memory:貴 要讀取兩次 所以通常有限制大小 CPU:快 可以放的不多 CPU會檢查d(offset)會不會超過limit,產生相對應的trap ¶Paging With TLB ¶Page Table and Virtual Address 如果要的address在TLB內的valid=0(沒有真的在memory內)就會觸發page fault,對應的handler會去memory找一塊physical memory並把這塊的address填到TLB內 ¶TLB Flush When Context Switch 通常要清掉TLB但是也可以設計成不要清掉 Q:利用TLB with PID info的技巧可以避免flush,那麼CPU multi-core時每個core有自己的TLB,kernel會不會嘗試儘量讓同個process排到同一個之前跑的core(當下可能是busy所以照樣等待)來避免switch到一個完全沒有先前TLB的core? A:linux會抉擇,會讓process有TLB的狀況下跑更快,直到TLB剩餘的加速能力比拿到新的core全部TLB miss後重新填回來還不划算時就會切換到新的core Is the TLB shared between multiple cores?[1] ¶Memory Cache Logical 讀0f50在logical cache中hit快速得到資料,此處為push指令 沒有hit就往TLB找 Pid or flush while context switch Physical Slow but share without flush 架構轉換原因之一為早期系統沒有很多process所以context switch沒那麼多 ¶Segmentation with paging (x86 example) 透過selector去table中選擇一個descripotor取得base和offset之後算出linear address在透過paging算出真正的physical address kernel可以看到全域的GDT的kernel code,data user透過LDT看到自己的code,data ¶User address space 4G for 32 bits processor How about kernel? Kernel uses another 4G address space ? Context switch for system calls? How about user and kernel share the same 4G spaces? User program can use X Kernel program can use 4G‐X Can user accesses the kernel data? Limited by user data segment Can kernel access the user data? Kernel data segment covers 4G ¶Physical memory management Always required OS has to know how physically memory is used Kernel (physical contiguous and logical contiguous addresses are preferred) For better performance Without MMU Low cost Better performance(不需要TLB 就不需要flush overhead) Deterministic performance(沒有virtual memory跟swapping space這樣access memory的時間較固定) ¶How to Protect Memory Access without MMU? MPU (Memory Protection Units) Low cost solution for multitasking and memory access protection 在CPU內部維護segment table Region Attributes Example 1 Example 2 What are the issues for physical memory management? External fragmentation Internal fragmentation Search for empty space ¶Fragmentation ¶External 如果用Memory packing去解決,在系統層面cost會太大 uCLinux Design[2] Standard Linux allocator: allocates blocks of 2n size. If 65 KB are requested, 128 KB will be reserved, and the remaining 63 KB won’t be reusable in Linux. uClinux 2.4 memory allocator: kmalloc2 (aka page_alloc2) Allocates blocks of 2n size until 4KB Uses 4KB pages for greater requests Stores amounts not greater than 8KB on the start of the memory, and larger ones at the end. Reduces fragmentation Not available yet for Linux 2.6. Frame Table Divide physical memory into frames – frame size = page size (why?) – Different frame size (why?) ¶Internal 當分配的size比frame size小時容易發生 Frame size跟table size本身是tradeoff Different frame size (why?) 在嵌入式系統可使用此技巧,藉由runtime分析之後去做最佳分配 ¶Kernel Memory Management ¶Requirements Managing memory have to spend memory Balance between management overhead and waste Prefer physical contiguous and logical contiguous addresses For better performance Different from application memory management (logical contiguous but not necessary physical contiguous) Memory utilization is more important Avoid external fragmentation Separate large and small allocations Avoid internal fragmentation Multiplexing more memory block requests into one page Support both large/small blocks allocations and free Large memory block such as DMA Small memory block such as task structure Lower empty space search complexities O(1) Provide APIs for kernel memory allocation/free For drivers, OS codes Provides APIs for realizing application memory allocation/free Allocate logical contiguous but not necessary physical contiguous memory ¶Overall Architecture Buddy System會去要實際的physical page,Slab Allocator目的是解決internal fragmentaion,然後最後才會mapping成user space看的VMA ¶System Components around Slab Allocaters Slab Allocator本身也是呼叫Page Allocator ¶Linux Memory Zones 通常分法 high:user normal:kernel Relationship Between Nodes, Zones and Pages strcut page對到physical frame ¶Linux Memory management Buddy allocation Why? O(1) 適合大塊記憶體 但是internal fragmentation嚴重 Zoned Buddy Allocator Slab allocation The Slab Allocator: An Object-Caching Kernel Memory Allocator[3] Slab allocators in the Linux Kernel[4] 讓好幾筆allocation用同個page 讓kernel program不用直接面對page allocator 把常alloc/free的object(structure) cache下來 避免每個kernel structure擺在太近造成cache line一直刷新 適合小塊記憶體 cache:同一類型的object slabs:連續的page組成page中放相同類型object Page to Cache and Slab Relationship Slab With Descriptor On‐Slab slab.c In order to reduce fragmentation, the slabs are sorted in 3 groups: full slabs with 0 free objects partial slabs empty slabs with no allocated objects If partial slabs exist, then new allocations come from these slabs, otherwise from empty slabs or new slabs are allocated. kmalloc similar to that of user‐space’s familiar malloc() routine byte‐sized chunks memory allocated is physically contiguous Through slab allocator vmalloc virtually contiguous and not necessarily physically contiguous user‐space allocation function works allocating potentially non-contiguous chunks of physical memory and “fixing up” the page tables to map the memory into a contiguous chunk of the logical address space ¶Process address space How The Kernel Manages Your Memory[5] Virtual Addresses Before/After Context Switches Per Process Address Space task_struct and mm_struct VMA 當使用library時可能會觸發page fault來把dll讀進memory Memory Access Page Fault ¶Linux Kernel Thread and mm_struct Kernel threads do not have a process address space mm field of a kernel thread’s process descriptor is NULL Lack of an address space is fine, because kernel threads do not ever access any user‐space memory Better performance ¶Management Regions mm_struct, vm_area_struct, and page VMA, Page and Frame Memory Region Red-Black Tree O(logn) Compare to AVL tree[6] Search slower Insert and delete faster Enlarge VMA ¶Memory mapping File mapping and memory layout Data Structure for File Memory Mapping ¶Two Types of Memory Mapping A file mapping maps a memory region to a region of a file backing store = file as long as the mapping is established, the content of the file can be read from or written to using direct memory access (“as if they were variables”) An anonymous mappings maps a memory region to a fresh “virtual” memory area filled with 0 backing store = zero‐ed memory area ¶Having memory mapped pages in common Thanks to virtual memory management, different processes can have mapped pages in common More precisely, mapped pages in different processes can refer to physical memory pages that have the same backing store That can happen in two ways: through fork, as memory mappings are inherited by children when multiple processes map the same region of a file ¶Shared vs private mappings With mapped pages in common, the involved processes might see changes performed by others to mapped pages in common, depending on whether the mapping is: private mapping in this case modifications are not visible to other processes. pages are initially the same, but modification are not shared, as it happens with copy‐on‐write memory after fork private mappings are also known as copy‐on‐write mappings shared mapping in this case modifications to mapped pages in common are visible to all involved processes pages are not copied‐on‐write ¶Shared file mapping Effects processes mapping the same region of a file share physical memory frames more precisely: they have virtual memory pages that map to the same physical memory frames additionally, the involved physical frames have the mapped file as ultimate backing store modifications to the (shared) physical frames are saved to the mapped file on disk Use cases memory‐mapped I/O, as an alternative to read/write as in the case of private file mapping, but here it works for both reading and writing data Inter‐process communication, with the following characteristics data‐transfer (not byte stream) with filesystem persistence among unrelated processes ¶Memory-mapped I/O Given that memory content is initialized from file changes to memory are reflected to file We can perform I/O by simply changing bytes of memory. Access to file mappings is less intuitive than sequential read/write operations the mental model is that of working on your data as a huge byte array (which is what memory is, after all) a best practice to follow is that of defining struct‐s that correspond to elements stored in the mapping, and copy them around with memcpy &amp; co ¶Advantages performance gain: 1 memory copy with read/write I/O each action involves 2 memory copies: between user‐space and kernel buffers between kernel and memory(buffer) of disk buffers and the I/O device with memory‐mapped I/O only the 2nd copy remains flash exercise: how many copies for standard I/O? performance gain: no context switch no syscall and no context switch is involved in accessing mapped memory page faults are possible, though reduced memory usage we avoid user‐space buffers ! less memory needed if memory mapped region is shared, we use only one set of buffers for all processes seeking is simplified no need of explicit lseek, just pointer manipulation ¶Disadvantages memory garbage the size of mapped regions is a multiple of system page size mapping regions which are way smaller than that can result in a significant waste of memory memory mapping must fit in the process address space on 32 bits systems, a large number of mappings of various sizes might result in memory fragmentation it then becomes harder to find continuous space to grant large memory mappings the problem is substantially diminished on 64 bits systems there is kernel overhead in maintaining mappings for small mappings, the overhead can dominate the advantages memory mapped I/O is best used with large files and random access ¶Page Cache Main Memory CS 4410, Operating Systems Fall 2016 Cornell University W4118: Linux memory management Programmation Systèmes Memory Mapping, Stefano Zacchiroli Is the TLB shared between multiple cores? ↩︎ Introduction to uClinux ↩︎ The Slab Allocator: An Object-Caching Kernel Memory Allocator ↩︎ Slab allocators in the Linux Kernel ↩︎ How The Kernel Manages Your Memory ↩︎ Red black tree over avl tree ↩︎]]></content>
      <tags>
        <tag>NCTU</tag>
        <tag>OSDI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Interrupt and exception handling]]></title>
    <url>%2F2018%2F05%2F13%2FInterrupt-and-exception-handling%2F</url>
    <content type="text"><![CDATA[¶Interrupt and exception handling This note is based on intel software development manual Volume 3A chap 6 ¶Interrupt and exception overview Interrupts occur at random times during the execution of a program, in response to signals from hardware. System hardware uses interrupts to handle events external to the processor, such as requests to service peripheral devices. Software can also generate interrupts by executing the INT n instruction. Exceptions occur when the processor detects an error condition while executing an instruction division by zero protection violations page faults internal machine faults ¶Exception and interrupt vectors The processor uses the vector number assigned to an exception or interrupt as an index into the interrupt descriptor table (IDT). The table provides the entry point to an exception or interrupt handler 0 through 31 are reserved 32 to 255 are designated as user-defined interrupts ¶Sources of interrupts External Interrupts External interrupts are received through pins on the processor or through the local APIC(Advanced Programmable Interrupt Controller) Maskable Hardware Interrupts Any external interrupt that is delivered to the processor by means of the INTR pin or through the local APIC is called a maskable hardware interrupt Software-Generated Interrupts The INT n instruction permits interrupts to be generated from within software by supplying an interrupt vector number as an operand. For example, the INT 35 instruction forces an implicit call to the interrupt handler for interrupt 35. ¶Source of exceptions Processor-detected program-error exceptions Software-generated exceptions The INTO, INT 3, and BOUND instructions permit exceptions to be generated in software. These instructions allow checks for exception conditions to be performed at points in the instruction stream. For example,INT 3 causes a breakpoint exception to be generated. Machine-check exceptions ¶Exception classifications Faults A fault is an exception that can generally be corrected and that, once corrected, allows the program to be restarted with no loss of continuity. When a fault is reported, the processor restores the machine state to the state prior to the beginning of execution of the faulting instruction. The return address (saved contents of the CS and EIP registers) for the fault handler points to the faulting instruction, rather than to the instruction following the faulting instruction. Traps A trap is an exception that is reported immediately following the execution of the trapping instruction. Traps allow execution of a program or task to be continued without loss of program continuity. The return address for the trap handler points to the instruction to be executed after the trapping instruction. If a trap is detected during an instruction which transfers execution, the return instruction pointer reflects the transfer. If a trap is detected while executing a JMP instruction, the return instruction pointer points to the destination of the JMP instruction, not to the next address past the JMP instruction. overflow exception is a trap exception. Here, the return instruction pointer points to the instruction following the INTO instruction that tested EFLAGS.OF (overflow) flag. The trap handler for this exception resolves the overflow condition. Upon return from the trap handler, program or task execution continues at the instruction following the INTO instruction. Aborts An abort is an exception that does not always report the precise location of the instruction causing the exception and does not allow a restart of the program or task that caused the exception. Aborts are used to report severe errors, such as hardware errors and inconsistent or illegal values in system tables. ¶Enabling and disabling interrupts The processor inhibits the generation of some interrupts, depending on the state of the processor and of the IF and RF flags in the EFLAGS register When the IF flag is clear, the processor inhibits interrupts delivered to the INTR pin or through the local APIC from generating an internal interrupt request when the IF flag is set, interrupts delivered to the INTR or through the local APIC pin are processed as normal external interrupts. The IF flag does not affect non-maskable interrupts (NMIs) delivered to the NMI pin or delivery mode NMI messages delivered through the local APIC The IF flag can be set or cleared with the STI (set interrupt-enable flag) and CLI (clear interrupt-enable flag) instructions ¶Interrupt descriptor table The interrupt descriptor table (IDT) associates each exception or interrupt vector with a gate descriptor for the procedure or task used to service the associated exception or interrupt. The base addresses of the IDT should be aligned on an 8-byte boundary to maximize performance of cache line fills. The limit value is expressed in bytes and is added to the base address to get the address of the last valid byte. A limit value of 0 results in exactly 1 valid byte. Because IDT entries are always eight bytes long, the limit should always be one less than an integral multiple of eight (that is, 8N – 1). The LIDT instruction loads the IDTR register with the base address and limit held in a memory operand. This instruction can be executed only when the CPL is 0. It normally is used by the initialization code of an operating system when creating an IDT ¶IDT DESCRIPTORS The task gate contains the segment selector for a TSS for an exception and/or interrupt handler task Interrupt and trap gates contain a far pointer (segment selector and offset) that the processor uses to transfer program execution to a handler procedure in an exception- or interrupt-handler code segment ¶Exception and interrupt handling An interrupt gate or trap gate references an exception- or interrupt-handler procedure that runs in the context of the currently executing task. The segment selector for the gate points to a segment descriptor for an executable code segment in either the GDT or the current LDT. The offset field of the gate descriptor points to the beginning of the exception- or interrupt-handling procedure. ¶Exception- or Interrupt-Handler Procedures When the processor performs a call to the exception- or interrupt-handler procedure: If the handler procedure is going to be executed at a numerically lower privilege level, a stack switch occurs. When the stack switch occurs: The segment selector and stack pointer for the stack to be used by the handler are obtained from the TSS for the currently executing task. On this new stack, the processor pushes the stack segment selector and stack pointer of the interrupted procedure. The processor then saves the current state of the EFLAGS, CS, and EIP registers on the new stack If an exception causes an error code to be saved, it is pushed on the new stack after the EIP value If the handler procedure is going to be executed at the same privilege level as the interrupted procedure The processor saves the current state of the EFLAGS, CS, and EIP registers on the current stack If an exception causes an error code to be saved, it is pushed on the current stack after the EIP value To return from an exception- or interrupt-handler procedure, the handler must use the IRET (or IRETD) instruction.The IRET instruction is similar to the RET instruction except that it restores the saved flags into the EFLAGS register ¶Protection The processor does not permit transfer of execution to an exception- or interrupt-handler procedure in a less privileged code segment than the CPL. The processor checks the DPL of the interrupt or trap gate only if an exception or interrupt is generated with an INT n, INT 3, or INTO instruction. Here, the CPL must be less than or equal to the DPL of the gate. prevents application programs or procedures running at privilege level 3 from using a software interrupt to access critical exception handlers, such as the page-fault handler Hardware-generated interrupts and processor-detected exceptions, the processor ignores the DPL of interrupt and trap gates.]]></content>
      <tags>
        <tag>OSDI</tag>
        <tag>note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OSDI-Process-Management]]></title>
    <url>%2F2018%2F04%2F19%2FOSDI-Process-Management%2F</url>
    <content type="text"><![CDATA[¶Process Management ¶tags: NCTU OSDI ¶Traps kernel CPU提供一些指令EX.interrupt,exception 使用者可以透過軟體中斷做一些處理 x86而言是透過software interrupt:int 0x80會將eax塞入對應的服務編號讓handler找到對應的function ¶x86 Interrupts and Linux Usage ¶Interrupts and exceptions 參考intel manual The processor provides two mechanisms for interrupting program execution, interrupts and exceptions: interrupt: an asynchronous event that is typically triggered by an I/O device. exception:a synchronous event that is generated when the processor detects one or more predefined conditions while executing an instruction. The IA-32 architecture specifies three classes of exceptions: faults, traps, and aborts. A trap is an exception that is reported immediately following the execution of the trapping instruction. Traps allow execution of a program or task to be continued without loss of program continuity. The return address for the trap handler points to the instruction to be executed after the trapping instruction. A fault is an exception that can generally be corrected and that, once corrected, allows the program to be restarted with no loss of continuity. When a fault is reported, the processor restores the machine state to the state prior to the beginning of execution of the faulting instruction. The return address (saved contents of the CS and EIP registers) for the fault handler points to the faulting instruction, rather than to the instruction following the faulting instruction. An abort is an exception that does not always report the precise location of the instruction causing the exception and does not allow a restart of the program or task that caused the exception. Aborts are used to report severe errors, such as hardware errors and inconsistent or illegal values in system tables. ¶System call CPU會先跳exception之後再跳system call Linux 一次進程的執行過程及一個ELF文件頭問題[1] 切換的同時stack pointer會改指到kernel stack ¶Kernel space vs user space Anatomy of a Program in Memory[2] ¶Program vs. Process ¶Process creation and destroy in Linux Process creation flow fork creates a child process that is a copy of the current task differs from the parent only in its PID, its PPID, and certain resources copy on write exec loads a new executable into the address space and begins executing it fork()+exec()=spawn() User process/thread create by parent run alternatively in Kernel Mode and in User Mode run kernel function through system calls Kernel process/thread normally create while OS booting run only in Kernel Mode run specific kernel function use only linear addresses (will be discussed later in MM) process 0: init (executed during start_kernel()) ¶Process descriptor Where Linux store the process descriptor? Of course memory In kernel space 把一塊(4k)拿來放process descriptor Per process (lightweight process) Allocated by slab allocator (will be described in MM) – why ? Co‐located with process kernel stack What is process kernel stack? A stack space used while process is running at kernel mode Why co‐located with process kernel stack? Easy to access by stack pointer How these process descriptors are managed Who &amp; when create process descriptor? “Parent” creates children &amp; process descriptor While (more precisely should be “before”) the process is created Who &amp; when destroy process descriptor? People destroy themselves, “Parent” or “ancestor” destroy the process descriptor While (more precisely should be “after”) the process is terminated Who &amp; when use process descriptor? Scheduler While scheduler is invoked ¶Process schedule and context switching in Linux Scheduling Find the next suitable process to run Context switch Store the context of the current process, restore the context of the next process When is the scheduler be invoked Direct invocation vs. Lazy invocation When returning to user‐space from a system call When returning to user‐space from an interrupt handler When an interrupt handler exits, before returning to kernel‐space If a task in the kernel explicitly calls schedule() If a task in the kernel blocks (which results in a call to schedule()) Timer interrupt 通常linux早期都是每秒1000次 tick kernel很耗電 現代OS已經不太採用 ¶Timer interrupt basics ¶Context ¶Process context 進kernel space不代表真的有context &quot;switch&quot;僅是mode的轉換 ¶Interrupt Context 遇到interrupt會保存當下部份的state(register等等)再進入interrupt context(overhead不會像context switch)那麼大 kernel space內能不能interrupt? 效率 vs 即時反應的取捨 ¶Example Flow ¶Scheduler呼叫時機 When returning to user-space from a system call EX:讀檔會需要等待,所以process自己設成idle 然後call scheduler If a task in the kernel blocks (which results in a call to schedule()) EX:call一個system call本身需要等待被lock住的kernel resource When returning to user‐space from an interrupt handler When an interrupt handler exits, before returning to kernel‐space If a task in the kernel explicitly calls schedule() ¶Preemption ¶User preemption Occurs when the kernel is in a safe state and about to return to user‐space ¶Kernel preemption Linux kernel is possible to preempt a task at any point, so long as the kernel does not hold a lock ¶Preemptive Kernel Non‐preemptive kernel supports user preemption Preemptive kernel supports kernel/user preemption Kernel can be interrupted != kernel is preemptive Non‐preemptive kernel, interrupt returns to interrupted process Preemptive kernel, interrupt returns to any schedulable process 2.4 is a non‐preemptive kernel 2.6 is a preemptive kernel 2.6 could disable CONFIG_PREEMPT 在被lock的過程中不能被preemption 沒被lock的情況下可以但是做完其他task必須回kernel繼續 ¶Single Core vs. Multi‐core 為何CPU可以同時執行同一段code但不能同時讀取同一塊memory? Ans:因為CPU有instruction cache ¶Linux scheduler example Timeslice function Type of Task Nice Value Timeslice Duration Initially created parent’s half of parent’s Minimum Priority +19 5ms(MIN_TIMESLICE) Default Priority 0 100ms(DEF_TIMESLICE ) Maximum Priority -20 800ms(MAX_TIMESLICE) ¶Process schedule and context switching in Linux Priority‐based scheduler Dynamic priority‐based scheduling Dynamic priority Normal process nice value: ‐20 to +19 (larger nice values imply you are being nice to others) Static priority Real‐time process 0 to 99 Total priority: 140 ¶Linux O(1) cheduler CPU利用find leading bit指令去找table中要schedule的task 原則就是先把priority高,有timeslice並且為running state的task做完 ¶Context switch Context switch Hardware context switch Task State Segment Descriptor (Old Linux) Step by step context switch Better control and optimize Context switch switch_mm() Switch virtual memory mapping switch_to() Switch processor state Process switching occurs only in kernel mode The contents of all registers used by a process in User Mode have already been saved ¶Hardware context switch How x86 helps in Context Switch Context Switching on x86[3] Context Switching[4] Linux 一次進程的執行過程及一個ELF文件頭問題 ↩︎ Anatomy of a Program in Memory ↩︎ Context Switching on x86 ↩︎ Context Switching ↩︎]]></content>
      <tags>
        <tag>NCTU</tag>
        <tag>OSDI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Office2010-on-Ubuntu-16.04]]></title>
    <url>%2F2018%2F04%2F13%2FOffice2010-on-Ubuntu-16-04%2F</url>
    <content type="text"><![CDATA[¶Office 2010 on Ubuntu 16.04 Install dependencies 1sudo apt install winbind mesa-utils mesa-utils-extra libgl1-mesa-glx:i386 libgl1-mesa-dev cabextract p7zip unzip wget zenity Install wine3.0 12345sudo dpkg --add-architecture i386wget -nc https://dl.winehq.org/wine-builds/Release.keysudo apt-key add Release.keysudo apt-add-repository &apos;deb https://dl.winehq.org/wine-builds/ubuntu/ xenial main&apos;sudo apt install --install-recommends winehq-stable download winetrick script 12wget https://raw.githubusercontent.com/Winetricks/winetricks/master/src/winetrickschmod +x winetricks Initialize wine in 32-bit mode 12export WINEARCH=win32winecfg In winecfg will lead you to installwine-mono and wine-gecko Install dotnet framework,office xml parser,windows font 1./winetricks dotnet20 msxml6 corefonts Mount iso and find the path to setup.exe and don’t open immediately after install 1wine /media/scps950707/OFFICE14/setup.exe Change settings for libraries after installing by running winecfg and add gdiplus and riched20 and change all of them to native In order to avoid lnk files created inside the same directory as the file opened, create the ~/.wine/drive_c/users/xxxx/Recent directory to let office put all lnk here The following steps are for KMS activation run regedit In key HKEY_LOCAL_MACHINE\Software\Microsoft\OfficeSoftwareProtectionPlatform add string values according to your kms server and port KeyManagementServiceName: kmserv.nctu.edu.tw KeyManagementServicePort: 1688 open WORD to check Wine will create lots of shortcut related to filetype which will pollute nautilus’s open with list, so there is a simple script to modify the its name 1234567891011121314151617#!/bin/shfiles="wine-*.desktop"for file in $filesdo # echo "$file" ext=$(echo "$file"|sed "s/wine-extension-//g;s/.desktop//g") # echo "EXT:$ext" name=$(grep "Name=" "$file"|sed "s/Name=//g") # echo "Name:$name" # echo "----------------" if [ "$name" = "Microsoft Excel" ] || [ "$name" = "Microsoft Word" ] \ || [ "$name" = "Microsoft PowerPoint" ] then echo "[$file]$name-&gt;$name""_""$ext" sed -i "s/$name/$name""_""$ext/" "$file" fidone Installing Office 2010 on Ubuntu 15.04 using Wine There is no public key available for the following key IDs 使用 Wine 安裝 Office 2010 於 Ubuntu 12.04 Activate Office 2010 running in PlayOnLinux with a KMS server Microsoft Office (installer only) Saving files in Microsoft Word/Excel 2000-2010 creates useless .lnk files]]></content>
      <tags>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OSDI-Booting-process]]></title>
    <url>%2F2018%2F03%2F02%2FOSDI-Booting-process%2F</url>
    <content type="text"><![CDATA[¶Booting process ¶tags: OSDI NCTU ¶Bootloader OverView ¶First loader Save in permanent memory such as ROM/NOR flash CPU會指定固定的位址到ROM取得取得data(開機要做的instruction) Check/initialize/test the hardware May provide basic services to other programs May provide shells to end‐users for basic operations Load/jump to the second loader (based on predefined procedures/configurations) Ex:u-boot for ARM, BIOS for x86 ¶Second loader As soon as first bootloader can access the second loader (on any device such as CD-ROM or hard disk), load it into memory, and can jump to the starting instruction Loading OS into memory Prepared by OS vendors or other third party vendors such as GRUB LILO ¶Third loader A program executes without OS services Initial and prepare OS services 像是vector table等等 ¶Fourth loader OS is now ready and can provide services Any process calls OS services to fork/execute other processes ¶Booting process overview ¶Power On 不是這門課關注的範圍詳情見[1][2] ¶Booting from permanent memory ¶ARM Example Hardware Example: FPU fire address到system bus 到Flash取得bootloader Procedure First Stage boot loader:設定/檢查硬體並且將CPU主導權交給second stage Second Stage boot loader:提供shell做一些設定,BIOS通常包含在此 First stage example [3][4][5] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899.globl _start/* Vector table 通常用來處理interrupt */_start: b reset b undefined_instruction b software_interrupt b prefetch_abort b data_abort b not_used b irq b fiq .../* the actual reset code */reset: /* First, mask **ALL** interrupts */ ldr r0, IC_BASE mov r1, #0x00 str r1, [r0, #ICMR] /* switch CPU to correct speed */ ldr r0, PWR_BASE LDR r1, cpuspeed str r1, [r0, #PPCR] /* setup memory */ bl memsetup /* init LED */ bl ledinit /* check if this is a wake-up from sleep */ ldr r0, RST_BASE // Reset status register ldr r1, [r0, #RCSR] and r1, r1, #0x0f teq r1, #0x08 bne normal_boot /* no, continue booting */ /* yes, a wake-up. clear RCSR by writing a 1 (see 9.6.2.1 from [1]) */ mov r1, #0x08 str r1, [r0, #RCSR] /* get value from the PSPR and jump to it */ ldr r0, PWR_BASE ldr r1, [r0, #PCSR] // Power manager scratch pad register mov pc, r1normal_boot: /* enable I-cache */ mrc p15, 0, r1, c1, c0, 0 @ read control reg orr r1, r1, #0x1000 @ set Icache mcr p15, 0, r1, c1, c0, 0 @ write it back /* check the first 1MB in increments of 4k */ mov r7, #0x1000 mov r6, r7, lsl #8 ldr r5, MEM_START /*清空1MB的RAM把second stage要用到的boot loader load到此處的RAM 因為需要較多的mem Read/Write,在ROM下效率不佳*/ mem_test_loop: mov r0, r5 bl testram teq r0, #1 beq badram add r5, r5, r7 subs r6, r6, r7 bne mem_test_loop /* the first megabyte is OK, so let's clear it */ mov r0, #((1024 * 1024) / (8 * 4)) // 1MB in steps of 32 bytes ldr r1, MEM_START...clear_loop: stmia r1!, &#123;r2-r9&#125; subs r0, r0, #(8 * 4) bne clear_loop /* relocate the second stage loader */ add r2, r0, #(128 * 1024) // blob is 128kB add r0, r0, #0x400 // skip first 1024 bytes ldr r1, MEM_START add r1, r1, #0x400 // skip over here as well...copy_loop: ldmia r0!, &#123;r3-r10&#125; stmia r1!, &#123;r3-r10&#125; cmp r0, r2 ble copy_loop /* set up the stack pointer */ ldr r0, MEM_START add r1, r0, #(1024*1024) sub sp, r1, #0x04 /* blob is copied to ram, so jump to it */ add r0, r0, #0x400 mov pc, r0 Second stage: 12345678910111213141516171819202122232425262728293031323334353637383940int main(void)&#123;~ led_on();~ SerialInit(baud9k6); TimerInit();~ SerialOutputString(PACKAGE " version " VERSION "Copyright (C) 1999 2000 2001 "~ get_memory_map();~ SerialOutputString("Running from "); if(RunningFromInternal()) SerialOutputString("internal"); else SerialOutputString("external");.../* wait 10 seconds before starting autoboot */SerialOutputString("Autoboot in progress, press any key..."); for(i = 0; i &lt; 10; i++) &#123; SerialOutputByte('.'); retval = SerialInputBlock(commandline, 1, 1);~ if(retval == 0) &#123;~ boot_linux(commandline); &#125;~ for(;;) &#123;~ if(numRead &gt; 0) &#123; if(MyStrNCmp(commandline, "boot", 4) == 0) &#123; boot_linux(commandline + 4); &#125; else if(MyStrNCmp(commandline, "clock", 5) == 0) &#123; SetClock(commandline + 5); &#125; else if(MyStrNCmp(commandline, "download ", 9) == 0) &#123;~ return 0;&#125; /* main */ ¶x86 Example Hardware First stage CPU一開機就會去抓FFFFFFF0h(ROM addr)的BIOS 將384KB的BIOS code從ROM搬到RAM並從0xFFFF0開始執行 Power supply sends POWER GOOD to CPU CPU resets Run System BIOS code at 0x0000~0xFFFF with respect to ROM address(But BIOS is already in RAM now) Jump to a real BIOS start address(通常不會隔太遠) POST(power on self test)，其中會檢查例如:是不是重開機，判斷的flag放在CMOS裡 Beep if there is an error Read CMOS data/settings 發出INT 19h去找開機硬碟，這檢查動作包含重硬碟load 1 sector(512bytes)到0x7c00並檢查最後兩個byte是不是0xAA55 主控權交給0x7c00這邊會準備load MBR [6][7][8] Second stage 呼叫interupt 13 service啟動&quot;hard disk drive service&quot; 到硬碟指定位置讀取MBR(Master Boot Record) ¶Booting from any devices 透過MBR再將GRUB或是LILO等OS loader載入memory並執行 KernelImage剖析 通常kernel image會再經過壓縮成為一個自解檔，此目的在於時間與空間的取捨。通常在嵌入式系統CPU較慢所以載入OS時偏好原始的kernel image ¶Loading OS Procedure Linux在開機的時候為了速度會使用Ramdisk將一部份driver載入並且存取,做完完整設定後再重新Remount真正的root filesystem[9][10][11][12] Example step 6 從floppy disk load MBR到0x7C00並複製一塊到0x90000並在那邊執行 Mount root disk Kernel Image Structure Kernel init procedure ¶Loading process ¶Booting speedup Remove waiting time Removing unnecessary initialization routines Uncompressed kernel DMA Copy Of Kernel On Startup Fast Kernel Decompression Kernel XIP(execution in place kernel run in ROM) How computer power supplies work – KitGuru Guide ↩︎ Everything You Need to Know About The Motherboard Voltage Regulator Circuit ↩︎ source code ↩︎ S3C2410 vivi閱讀筆記 ↩︎ linux kernel driver for s3c2410 ↩︎ BIOS ↩︎ UEFI ↩︎ INT 19H Bootstrap Loader ↩︎ Jserv’s blog: 探索 Linux bootloader 的佳作 ↩︎ Jserv’s blog: 深入理解 Linux 2.6 的 initramfs 機制 ↩︎ 嵌入式系统 Boot Loader 技术内幕 ↩︎ initrd和initramfs的區別 ↩︎]]></content>
      <tags>
        <tag>NCTU</tag>
        <tag>OSDI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android Bitmap 解析]]></title>
    <url>%2F2018%2F02%2F05%2FAndroid_Bitmap%2F</url>
    <content type="text"><![CDATA[¶Android Bitmap 解析 本篇文章基於8.0.0_r1的AOSP ¶copy 1234567public Bitmap copy(Config config, boolean isMutable) &#123; checkRecycled("Can't copy a recycled bitmap"); ... Bitmap b = nativeCopy(mNativePtr, config.nativeInt, isMutable); ... return b; &#125; copy對應的JNI實作在[1]中呼叫createBitmap產生java object,詳細請見createBitmap 12345678910111213141516static jobject Bitmap_copy(JNIEnv* env, jobject, jlong srcHandle, jint dstConfigHandle, jboolean isMutable) &#123; SkBitmap src; reinterpret_cast&lt;BitmapWrapper*&gt;(srcHandle)-&gt;getSkBitmap(&amp;src); ...... SkColorType dstCT = GraphicsJNI::legacyBitmapConfigToColorType(dstConfigHandle); SkBitmap result; HeapAllocator allocator; if (!bitmapCopyTo(&amp;result, dstCT, src, &amp;allocator)) &#123; return NULL; &#125; auto bitmap = allocator.getStorageObjAndReset(); return createBitmap(env, bitmap, getPremulBitmapCreateFlags(isMutable));&#125; bitmapCopyTo的實現同在[1:1]中,設定了新的SkBitmap的特性 並且呼叫[2]中的copyTo讓skia完成SkBitmap最底層的copy 1234567891011121314151617181920static bool bitmapCopyTo(SkBitmap* dst, SkColorType dstCT, const SkBitmap&amp; src, SkBitmap::Allocator* alloc) &#123; ... SkImageInfo dstInfo = src.info().makeColorType(dstCT); if (dstCT == kRGBA_F16_SkColorType) &#123; dstInfo = dstInfo.makeColorSpace(SkColorSpace::MakeSRGBLinear()); &#125; if (!dst-&gt;setInfo(dstInfo)) &#123;return false;&#125; if (!dst-&gt;tryAllocPixels(alloc, nullptr)) &#123;return false;&#125; switch (dstCT) &#123; case kRGBA_8888_SkColorType: case kBGRA_8888_SkColorType: &#123;...&#125; case kRGB_565_SkColorType: &#123;...&#125; case kRGBA_F16_SkColorType: &#123;...&#125; default: return false; &#125; &#125; return src.copyTo(dst, dstCT, alloc);&#125; ¶createBitmap Android Bitmap和Canvas綁定做使用並透過createBitmap產生 12Bitmap b = Bitmap.createBitmap(400, 400, Bitmap.Config.ARGB_8888); Canvas canvas = new Canvas(b); 其中createBitmap會呼叫[3]中的 123456789public static Bitmap createBitmap(@NonNull DisplayMetrics display, @NonNull @ColorInt int[] colors, int offset, int stride, int width, int height, @NonNull Config config) &#123; ... Bitmap bm = nativeCreate(colors, offset, stride, width, height, config.nativeInt, false, null, null); ... return bm; &#125; nativeCreate對應的JNI實作在[1:2] 1234567891011121314151617static jobject Bitmap_creator(JNIEnv* env, jobject, jintArray jColors, jint offset, jint stride, jint width, jint height, jint configHandle, jboolean isMutable, jfloatArray xyzD50, jobject transferParameters) &#123; SkColorType colorType = GraphicsJNI::legacyBitmapConfigToColorType(configHandle); ... SkBitmap bitmap; ... bitmap.setInfo(SkImageInfo::Make(width, height, colorType, kPremul_SkAlphaType, colorSpace)); sk_sp&lt;Bitmap&gt; nativeBitmap = Bitmap::allocateHeapBitmap(&amp;bitmap, NULL); if (!nativeBitmap) &#123; return NULL; &#125; ... return createBitmap(env, nativeBitmap.release(), getPremulBitmapCreateFlags(isMutable));&#125; 123sk_sp&lt;Bitmap&gt; Bitmap::allocateHeapBitmap(SkBitmap* bitmap, SkColorTable* ctable) &#123; return allocateBitmap(bitmap, ctable, &amp;android::allocateHeapBitmap);&#125; 最終的android::allocateHeapBitmap如下 12345678static sk_sp&lt;Bitmap&gt; allocateHeapBitmap(size_t size, const SkImageInfo&amp; info, size_t rowBytes, SkColorTable* ctable) &#123; void* addr = calloc(size, 1); if (!addr) &#123; return nullptr; &#125; return sk_sp&lt;Bitmap&gt;(new Bitmap(addr, size, info, rowBytes, ctable));&#125; allocateHeapBitmap又將android::allocateHeapBitmap這個function的function pointer傳給allocateBitmap 透過alloc指到的function建立bitmap並回傳 1234567891011static sk_sp&lt;Bitmap&gt; allocateBitmap(SkBitmap* bitmap, SkColorTable* ctable, AllocPixeRef alloc) &#123; ... auto wrapper = alloc(size, info, rowBytes, ctable); if (wrapper) &#123; wrapper-&gt;getSkBitmap(bitmap); // since we're already allocated, we lockPixels right away // HeapAllocator behaves this way too bitmap-&gt;lockPixels(); &#125; return wrapper;&#125; Storage除了heap外,[4]:還提供了三種方法 external:傳入外部address和對應的free function pointer 12345struct &#123;void* address;void* context;FreeFunc freeFunc;&#125; external; ashmem:呼叫[5]中的Bitmap::allocateAshmemBitmap使用mmap建立Shared memory hardware:呼叫[5:1]中的Bitmap::allocateHardwareBitmap建立GraphicBuffer 最後會returncreateBitmap,實作同在[1:3]中 123456789101112131415jobject createBitmap(JNIEnv* env, Bitmap* bitmap, int bitmapCreateFlags, jbyteArray ninePatchChunk, jobject ninePatchInsets, int density) &#123; ... BitmapWrapper* bitmapWrapper = new BitmapWrapper(bitmap); jobject obj = env-&gt;NewObject(gBitmap_class, gBitmap_constructorMethodID, reinterpret_cast&lt;jlong&gt;(bitmapWrapper), bitmap-&gt;width(), bitmap-&gt;height(), density, isMutable, isPremultiplied, ninePatchChunk, ninePatchInsets); if (env-&gt;ExceptionCheck() != 0) &#123; ALOGE("*** Uncaught exception returned from Java call!\n"); env-&gt;ExceptionDescribe(); &#125; return obj;&#125; 此處會透過JNI在將skia建立好的native bitmap address傳回[3:1]的constructor,以jlong直接存在Bitmap.java中mNativePtr中 123456789101112131415161718Bitmap(long nativeBitmap, int width, int height, int density, boolean isMutable, boolean requestPremultiplied, byte[] ninePatchChunk, NinePatch.InsetStruct ninePatchInsets) &#123; ... mWidth = width; mHeight = height; mIsMutable = isMutable; mRequestPremultiplied = requestPremultiplied; mNinePatchChunk = ninePatchChunk; mNinePatchInsets = ninePatchInsets; if (density &gt;= 0) &#123; mDensity = density; &#125; mNativePtr = nativeBitmap; ...&#125; ¶recycle 回收機制本身是非同步的 12345678public void recycle() &#123;if (!mRecycled &amp;&amp; mNativePtr != 0) &#123; if (nativeRecycle(mNativePtr)) &#123; mNinePatchChunk = null; &#125; mRecycled = true; &#125;&#125; JNI的實作在[1:4]中 123456789101112131415161718192021static jboolean Bitmap_recycle(JNIEnv* env, jobject, jlong bitmapHandle) &#123; LocalScopedBitmap bitmap(bitmapHandle); bitmap-&gt;freePixels(); return JNI_TRUE;&#125;class BitmapWrapper &#123;public: BitmapWrapper(Bitmap* bitmap) : mBitmap(bitmap) &#123; &#125; void freePixels() &#123; mInfo = mBitmap-&gt;info(); mHasHardwareMipMap = mBitmap-&gt;hasHardwareMipMap(); mAllocationSize = mBitmap-&gt;getAllocationByteCount(); mRowBytes = mBitmap-&gt;rowBytes(); mGenerationId = mBitmap-&gt;getGenerationID(); mIsHardware = mBitmap-&gt;isHardware(); mBitmap.reset(); &#125;&#125; 最後呼叫skia SkBitmap[2:1]的reset呼叫sk_bzero將bitmapinstance清空 ¶Ref Bitmap Android6.0 Bitmap存储以及Parcel传输源码分析 frameworks/base/core/jni/android/graphics/Bitmap.cpp ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ external/skia/src/core/SkBitmap.cpp ↩︎ ↩︎ frameworks/base/graphics/java/android/graphics/Bitmap.java ↩︎ ↩︎ frameworks/base/libs/hwui/hwui/Bitmap.h ↩︎ frameworks/base/libs/hwui/hwui/Bitmap.cpp ↩︎ ↩︎]]></content>
      <tags>
        <tag>Android</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C and C++ notes]]></title>
    <url>%2F2018%2F02%2F01%2Fcppnote%2F</url>
    <content type="text"><![CDATA[¶C/C++ ¶Bit field Bit Fields allow the packing of data in a structure. This is especially useful when memory or data storage is at a premium. Typical examples: Packing several objects into a machine word. e.g. 1 bit flags can be compacted – Symbol tables in compilers. Reading external file formats – non-standard file formats could be read in. E.g. 9 bit integers. ¶Explanation[1] The number of bits in a bit field sets the limit to the range of values it can hold: 1234567891011struct S&#123;// three-bit unsigned field,allowed values are 0...7 unsigned int b : 3;&#125;;int main()&#123; S s = &#123;7&#125;; ++s.b; // unsigned overflow (guaranteed wrap-around) std::cout &lt;&lt; s.b &lt;&lt; '\n'; // output: 0&#125; Multiple adjacent bit fields are usually packed together (although this behavior is implementation-defined): 1234567891011121314struct S&#123; // will usually occupy 2 bytes: // 3 bits: value of b1 // 2 bits: unused // 6 bits: value of b2 // 2 bits: value of b3 // 3 bits: unused unsigned char b1 : 3, : 2, b2 : 6, b3 : 2;&#125;;int main()&#123; std::cout &lt;&lt; sizeof( S ) &lt;&lt; '\n'; // usually prints 2&#125; The special unnamed bit field of size zero can be forced to break up padding. It specifies that the next bit field begins at the beginning of its allocation unit: 123456789101112131415struct S &#123; // will usually occupy 2 bytes: // 3 bits: value of b1 // 5 bits: unused // 6 bits: value of b2 // 2 bits: value of b3 unsigned char b1 : 3; unsigned char :0; // start a new byte unsigned char b2 : 6; unsigned char b3 : 2;&#125;;int main()&#123; std::cout &lt;&lt; sizeof(S) &lt;&lt; '\n'; // usually prints 2/* &#125; */ The following properties of bit fields are implementation-defined The value that results from assigning or initializing a signed bit field with a value out of range, or from incrementing a signed bit field past its range. Everything about the actual allocation details of bit fields within the class object For example, on some platforms, bit fields don’t straddle bytes, on others they do Also, on some platforms, bit fields are packed left-to-right, on others right-to-left It is implementation-defined whether a plain (neither explicitly signed nor unsigned) char, short, int, long, or long long bit-field is signed or unsigned. ¶Friend ¶Definition[2] The friend declaration appears in a class body and grants a function or another class access to private and protected members of the class where the friend declaration appears. ¶Example Designates a function or several functions as friends of this class 12345678910111213class Y &#123; int data; // private member // the non-member function operator&lt;&lt; will have access to Y's private members friend std::ostream&amp; operator&lt;&lt;(std::ostream&amp; out, const Y&amp; o); friend char* X::foo(int); // members of other classes can be friends too friend X::X(char), X::~X(); // constructors and destructors can be friends&#125;;// friend declaration does not declare a member function// this operator&lt;&lt; still needs to be defined, as a non-memberstd::ostream&amp; operator&lt;&lt;(std::ostream&amp; out, const Y&amp; y)&#123; return out &lt;&lt; y.data; // can access private member Y::data&#125; (only allowed in non-local class definitions) Defines a non-member function, and makes it a friend of this class at the same time. Such non-member function is always inline. 12345678910class X &#123; int a; friend void friend_set(X&amp; p, int i) &#123; p.a = i; // this is a non-member function &#125; public: void member_set(int i) &#123; a = i; // this is a member function &#125;&#125;; ¶Notes Friendship is not transitive (a friend of your friend is not your friend) Friendship is not inherited (your friend’s children are not your friends) When should you use ‘friend’ in C++?[3] ¶Linkage[4][5] Linkage describes how identifiers can or can not refer to the same entity throughout the whole program or one single translation unit. A translation unit is a source file plus all the headers you #included in it, from which the compiler creates the object file. ¶Internal linkage Internal linkage means that storage is created to represent the identifier only for the translation unit being compiled. Other translation units may use the same identifier name with internal linkage, or for a global variable, and no conflicts will be found by the linker because separate storage is created for each identifier Any of the following names declared at namespace scope have internal linkage variables, functions, or function templates declared static data members of anonymous unions ¶External linkage External linkage means that a single piece of storage is created to represent the identifier for all translation units being compiled. The storage is created once, and the linker must resolve all other references to that storage. Identifiers with external linkage can be accessed from other translation units by declaring them with the keyword extern. ¶No linkage The name can be referred to only from the scope it is in. Any of the following names declared at block scope have no linkage: Variables that aren’t explicitly declared extern (regardless of the static modifier) Local classes and their member functions Other names declared at block scope such as typedefs, enumerations, and enumerators ¶Storage class specifiers[4:1][5:1] ¶Auto The auto specifier was only allowed for objects declared at block scope or in function parameter lists. It indicated automatic storage duration, which is the default for these kinds of declarations. The meaning of this keyword was changed in C++11 ¶Register The register storage class is used to define local variables that should be stored in a register instead of RAM (if possible) (for faster access speed). This means that the variable has a maximum size equal to the register size (usually one word) and can’t have the unary &amp; operator applied to it (as it does not have a memory location). However, there is no guarantee that the variable will be placed in a register or even that the access speed will increase. It is just a hint to the compiler, this keyword was deprecated in C++11. ¶Static A static global variable or static function at file scope specifies that the variable or function has internal linkage. A static variable in a function specifies that the variable retains its state between calls to that function. Variables declared at block scope with the specifier static have static storage duration but are initialized the first time control passes through their declaration (unless their initialization is zero- or constant-initialization, which can be performed before the block is first entered). On all further calls, the declaration is skipped. A static data member in a class declaration specifies that one copy of the member is shared by all instances of the class. A static data member must be defined at file scope. An integral data type member[6] that you declare as const static can have an initializer.[7] 1234567891011121314151617181920#include &lt;iostream&gt;using namespace std;class A&#123;public: static int num;/* just a declaration */ static const int num2 = 5;// const static integral type can have initializer /* static int num3 = 3;//complier error */ /* non-const static data member nust be initialized out of line */&#125;;int A::num = 1;//The member shall still be defined in file scopeint main( void )&#123; cout &lt;&lt; A::num &lt;&lt; endl;// output 1 cout &lt;&lt; A::num2 &lt;&lt; endl;// output 5 return 0;&#125; When you declare a member function in a class declaration, the static keyword specifies that the function is shared by all instances of the class. A static member function cannot access an instance member because the function does not have an implicit this pointer. To access an instance member, declare the function with a parameter that is an instance pointer or reference. You cannot declare the members of a union as static. However, a globally declared anonymous union must be explicitly declared static. ¶Extern The extern specifier is only allowed in the declarations of variables and functions (except class members or function parameters). It specifies external linkage, and does not technically affect storage duration, but it cannot be used in a definition of an automatic storage duration object, so all extern objects have static or thread durations. In addition, a variable declaration that uses extern and has no initializer is not a definition. ¶Casting[8][9][10] ¶Explicit type casting Explicit type casting is a way of explicitly informing the compiler that you intend to make the conversion and that you are aware that data loss might occur ¶C style casting A C-style cast is defined as the first of the following which succeeds: const_cast static_cast&lt;new_type&gt;(expression) static_cast (with extensions) followed by const_cast; reinterpret_cast&lt;new_type&gt;(expression); reinterpret_cast followed by const_cast It can therefore be used as a replacement for other casts in some instances, but can be extremely dangerous because of the ability to devolve into a reinterpret_cast, and the latter should be preferred when explicit casting is needed, unless you are sure static_cast will succeed or reinterpret_cast will fail. Even then, consider the longer, more explicit option. ¶static_cast static_cast can perform conversions between pointers to related classes, not only upcasts (from pointer-to-derived to pointer-to-base), but also downcasts (from pointer-to-base to pointer-to-derived). No checks are performed during runtime to guarantee that the object being converted is in fact a full object of the destination type. Therefore, it is up to the programmer to ensure that the conversion is safe. On the other side, it does not incur the overhead of the type-safety checks of dynamic_cast. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566#include &lt;vector&gt;#include &lt;iostream&gt;struct B &#123; int m = 0; void hello() const &#123; std::cout &lt;&lt; "Hello world, this is B!\n"; &#125;&#125;;struct D : B &#123; void hello() const &#123; std::cout &lt;&lt; "Hello world, this is D!\n"; &#125;&#125;;enum class E &#123; ONE = 1, TWO, THREE &#125;;enum EU &#123; ONE = 1, TWO, THREE &#125;;int main()&#123; // 1: initializing conversion int n = static_cast&lt;int&gt;(3.14); std::cout &lt;&lt; "n = " &lt;&lt; n &lt;&lt; '\n'; std::vector&lt;int&gt; v = static_cast&lt;std::vector&lt;int&gt;&gt;(10); std::cout &lt;&lt; "v.size() = " &lt;&lt; v.size() &lt;&lt; '\n'; // 2: static downcast D d; B&amp; br = d; // upcast via implicit conversion br.hello(); D&amp; another_d = static_cast&lt;D&amp;&gt;(br); // downcast another_d.hello(); // 3: lvalue to xvalue std::vector&lt;int&gt; v2 = static_cast&lt;std::vector&lt;int&gt;&amp;&amp;&gt;(v); std::cout &lt;&lt; "after move, v.size() = " &lt;&lt; v.size() &lt;&lt; '\n'; // 4: discarded-value expression static_cast&lt;void&gt;(v2.size()); // 5. inverse of implicit conversion void* nv = &amp;n; int* ni = static_cast&lt;int*&gt;(nv); std::cout &lt;&lt; "*ni = " &lt;&lt; *ni &lt;&lt; '\n'; // 6. array-to-pointer followed by upcast D a[10]; B* dp = static_cast&lt;B*&gt;(a); // 7. scoped enum to int or float E e = E::ONE; int one = static_cast&lt;int&gt;(e); std::cout &lt;&lt; one &lt;&lt; '\n'; // 8. int to enum, enum to another enum E e2 = static_cast&lt;E&gt;(one); EU eu = static_cast&lt;EU&gt;(e2); // 9. pointer to member upcast int D::*pm = &amp;D::m; std::cout &lt;&lt; br.*static_cast&lt;int B::*&gt;(pm) &lt;&lt; '\n'; // 10. void* to any type void* voidp = &amp;e; std::vector&lt;int&gt;* p = static_cast&lt;std::vector&lt;int&gt;*&gt;(voidp);&#125; ¶dynamic_cast dynamic_cast can only be used with pointers and references to classes (or with void*). Its purpose is to ensure that the result of the type conversion points to a valid complete object of the destination pointer type. This naturally includes pointer upcast (converting from pointer-to-derived to pointer-to-base), in the same way as allowed as an implicit conversion. But dynamic_cast can also downcast (convert from pointer-to-base to pointer-to-derived) polymorphic classes (those with virtual members) if -and only if- the pointed object is a valid complete object of the target type 12345678910111213141516171819202122#include &lt;iostream&gt;#include &lt;exception&gt;using namespace std;class Base &#123; virtual void dummy() &#123;&#125; &#125;;class Derived: public Base &#123; int a; &#125;;int main () &#123; try &#123; Base * pba = new Derived; Base * pbb = new Base; Derived * pd; pd = dynamic_cast&lt;Derived*&gt;(pba); if (pd==0) cout &lt;&lt; "Null pointer on first type-cast.\n"; pd = dynamic_cast&lt;Derived*&gt;(pbb); if (pd==0) cout &lt;&lt; "Null pointer on second type-cast.\n"; &#125; catch (exception&amp; e) &#123;cout &lt;&lt; "Exception: " &lt;&lt; e.what();&#125; return 0;&#125; ¶reinterpret_cast reinterpret_cast turns one type directly into another - such as casting the value from one pointer to another, or storing a pointer in an int, or all sorts of other nasty things.It’s used primarily for particularly weird conversions and bit manipulations, like turning a raw data stream into actual data, or storing data in the low bits of an aligned pointer. 1234567891011#include&lt;stdio.h&gt;#include&lt;stdint.h&gt;int main( void )&#123; int tmp = 1; printf("%p\n",&amp;tmp);// print address of tmp int64_t addr = reinterpret_cast&lt;int64_t&gt;(&amp;tmp); // force complier to cast tmp's address to 64bit integer value printf("%lx\n",addr); // on my computer(64bit) both print "7fffc56e09dc"&#125; ¶const_cast[11] used to add/remove const(ness) (or volatile-ness) of a variable. Although const cast allows the value of a constant to be changed, doing so is still invalid code that may cause a run-time error. This could occur for example if the constant was located in a section of read-only memory. Const cast is instead used mainly when there is a function that takes a non-constant pointer argument, even though it does not modify the pointee.The function can then be passed a constant variable by using a const cast. 1234567891011121314#include&lt;stdio.h&gt;void fucn(int *p)&#123; printf("%d",*p);&#125;int main( void )&#123; const int myConst=5; int *nonConst = const_cast&lt;int*&gt;(&amp;myConst);// remove const attribute /* *nonConst = 10; potential run-time error */ /* fucn(&amp;myConst); error: cannot convert const int* to int* */ fucn(nonConst); return 0;&#125; ¶Const[12][13] ¶basic usage ¶a pointer which points to constant variable const int *ptr or int const *ptr ¶a constant pointer which points to variable int * const ptr ¶a constant pointer which points to constant variable int const * const ptr ¶const member functions Declaring a member function with the const keyword specifies that the function is a “read-only” function that does not modify the object for which it is called. A constant member function cannot modify any non-static data members or call any member functions that aren’t constant. To declare a constant member function, place the const keyword after the closing parenthesis of the argument list. The const keyword is required in both the declaration and the definition 1234567891011121314class foo&#123;public: foo(int x,int y):data(x)&#123;&#125; void func() const;private: int data;&#125;;void foo::func() const&#123; this-&gt;data= 3; // error:can't assign to non-static data member within const member cout &lt;&lt; data &lt;&lt; endl;&#125; ¶const return values 12345678910#include&lt;stdio.h&gt;const char* func()&#123; return "hello world";&#125;int main( void )&#123; func()[0]='a';//could cause write data into read only memory, use const to let complier warn programer return 0;&#125; ¶parameter passing tip call by value makes copy of parameter, in C++ we can use call by reference simply pass reference(address) to save the time for copying data, so we can use const to prevent data modified.For example void func(int &amp;a). ¶64-bit data models[14] Data model short (integer) int long (integer) long long pointers,size_t Sample operating systems LLP64 16 32 32 64 64 Microsoft Windows (x86-64 and IA-64) using Visual C++; and MinGW LP64 16 32 64 64 64 Most Unix and Unix-like systems, e.g., Solaris, Linux, BSD, macOS. Windowswhen using Cygwin; z/OS ILP64 16 64 64 64 64 HAL Computer Systems port of Solaris to the SPARC64 SILP64 64 64 64 64 64 Classic UNICOS[40] (versus UNICOS/mp, etc.) Some experiment 1234567891011121314#include&lt;stdio.h&gt;#define TYPESIZE(a) printf(#a&quot;:%d\n&quot;,(int)sizeof(a))int main( int argc, char *argv[] )&#123; TYPESIZE(char); TYPESIZE(short); TYPESIZE(int); TYPESIZE(long); TYPESIZE(long long); TYPESIZE(float); TYPESIZE(double); TYPESIZE(void*); return 0;&#125; 12345678910111213141516171819202122$ gcc -m32 test.c &amp;&amp; ./a.outchar:1short:2int:4long:4long long:8float:4double:8void*:4$file a.outa.out: ELF 32-bit LSB executable, Intel 80386$ gcc -m64 test.c &amp;&amp; ./a.outchar:1short:2int:4long:8long long:8float:4double:8void*:8$file a.outa.out: ELF 64-bit LSB executable, x86-64 ¶Alignment[15] The type of each member of the structure usually has a default alignment, meaning that it will, unless otherwise requested by the programmer, be aligned on a pre-determined boundary. The following typical alignments are valid for compilers from Microsoft (Visual C++), Borland/CodeGear (C++Builder), Digital Mars (DMC), and GNU (GCC) when compiling for 32-bit x86 A char (one byte) will be 1-byte aligned. A short (two bytes) will be 2-byte aligned. An int (four bytes) will be 4-byte aligned. A long (four bytes) will be 4-byte aligned. A float (four bytes) will be 4-byte aligned. A double (eight bytes) will be 8-byte aligned on Windows and 4-byte aligned on Linux (8-byte with -malign-double compile time option). A long long (eight bytes) will be 4-byte aligned. A long double (ten bytes with C++ Builder and DMC, eight bytes with Visual C++, twelve bytes with GCC) will be 8-byte aligned with C++ Builder, 2-byte aligned with DMC, 8-byte aligned with Visual C++, and 4-byte aligned with GCC. Any pointer (four bytes) will be 4-byte aligned. (e.g.: char*, int*) The only notable differences in alignment for an LP64 64-bit system when compared to a 32-bit system are: A long (eight bytes) will be 8-byte aligned. A double (eight bytes) will be 8-byte aligned. A long long (eight bytes) will be 8-byte aligned. A long double (eight bytes with Visual C++, sixteen bytes with GCC) will be 8-byte aligned with Visual C++ and 16-byte aligned with GCC. Any pointer (eight bytes) will be 8-byte aligned. cppreference bit_field ↩︎ cppreference friend ↩︎ When should you use ‘friend’ in C++? ↩︎ Storage class specifiers ↩︎ ↩︎ C++: auto / register / static / const / volatile / linkage / scope ↩︎ ↩︎ Fundamental Types (C++) ↩︎ static variable in the class declaration or definition? ↩︎ C++ 的顯性轉型與隱性轉型 - Explicitly/ImplicitlyType Conversion ↩︎ When should static_cast, dynamic_cast, const_cast and reinterpret_cast be used? ↩︎ Type conversions ↩︎ Regular cast vs. static_cast vs. dynamic_cast ↩︎ C++學習筆記 常數 Const ↩︎ The C++ ‘const’ Declaration: Why &amp; How ↩︎ 64-bit computing ↩︎ Data_structure_alignment ↩︎]]></content>
      <tags>
        <tag>note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Operating System Note]]></title>
    <url>%2F2018%2F02%2F01%2Fos%2F</url>
    <content type="text"><![CDATA[¶Operating System ¶Thread ¶Property[1] Light-weighted threads switch between threads but not between virtual memories Threads allow user programs to continue after starting I/O Each thread can run in parallel on a different processor Extensive sharing makes CPU switching among peer threads and creation of threads inexpensive compared to processes Thread context switch still requires Register set switch No memory management related work!!! ¶Compare to process Per process items Per thread items Address space Program counter Global variables Registers Open files Stack Child processes State Pending alarms Signals and signal handlers Accounting information ¶User-level Threads(pthread) User-level threads implement in user-level libraries, rather than via systems calls, so thread switching does not need to call operating system and to cause interrupt to the kernel. In fact, the kernel knows nothing about user-level threads and manages them as if they were single-threaded processes. ¶Advantages User-level threads does not require modification to operating systems. Each thread is represented simply by a PC, registers, stack and a small control block, all stored in the user process address space. The procedure that saves the local thread state and the scheduler are local procedures, hence no trap to kernel, no context switch, no memory switch, and this makes the thread scheduling very fast. ¶Disadvantages There is a lack of coordination between threads and operating system kernel. Therefore, process as whole gets one time slice irrespect of whether process has one thread or 1000 threads within. It is up to each thread to relinquish control to other threads. User-level threads requires non-blocking systems call i.e., a multithreaded kernel. Otherwise, entire process will blocked in the kernel, even if there are runable threads left in the processes. For example, if one thread causes a page fault, the process blocks. ¶Kernel-Level Threads In this method, the kernel knows about and manages the threads. No runtime system is needed in this case. Instead of thread table in each process, the kernel has a thread table that keeps track of all threads in the system. In addition, the kernel also maintains the traditional process table to keep track of processes. Operating Systems kernel provides system call to create and manage threads. ¶Advantages Because kernel has full knowledge of all threads, Scheduler may decide to give more time to a process having large number of threads than process having small number of threads. Kernel-level threads are especially good for applications that frequently block. ¶Disadvantages The kernel-level threads are slow and inefficient. For instance, threads operations are hundreds of times slower than that of user-level threads. Since kernel must manage and schedule threads as well as processes. It require a full thread control block (TCB) for each thread to maintain information ¶Multi-threading Models ¶Many-to-One Model many user threads are mapped to one kernel thread ¶Advantage thread management is done in user space, so it is efficient ¶Disadvantage Entire process will block if a thread makes a blocking call to the kernel Because only one thread can access kernel at a time, no parallelism on multiprocessors is possible ¶One-to-One Model one user thread maps to kernel thread ¶Advantage more concurrency than in many-to-one model Multiple threads can run in parallel on multi-processors ¶Disadvantage Creating a user thread requires creating the corresponding kernel thread. There is an overhead related with creating kernel thread which can be burden on the performance. ¶Many-to-Many Model many user threads are multiplexed onto a smaller or equal set of kernel threads. ¶Advantage Application can create as many user threads as wanted Kernel threads run in parallel on multiprocessors When a thread blocks, another thread can still run ¶Linux implementations of POSIX threads[2] Over time, two threading implementations have been provided by the GNU C library on Linux: ¶LinuxThreads This is the original Pthreads implementation. Since glibc 2.4, this implementation is no longer supported. ¶NPTL (Native POSIX Threads Library) This is the modern Pthreads implementation. By comparison with LinuxThreads, NPTL provides closer conformance to the requirements of the POSIX.1 specification and better performance when creating large numbers of threads. NPTL is available since glibc 2.3.2, and requires features that are present in the Linux 2.6 kernel. Both of these are so-called 1:1 implementations, meaning that each thread maps to a kernel scheduling entity. Both threading implementations employ the Linux clone system call. In NPTL, thread synchronization primitives (mutexes, thread joining, and so on) are implemented using the Linux futex system call. ¶Semaphore Consider a Semaphore S,the following operations done atomically(hardware support) ¶DOWN(P,wait,sleep) 1234if(S.value&gt;0) S.value=S.value-1;else Add process to queue(sleep,doesn't finish DOWN operation) ¶UP(V,wakeup,signal) 123S.value=S.value+1;if(one or more process were sleeping at that semaphore) wakeup process from queue,the process finish DOWN operation Thus, after an an up on a semaphore with process sleeping on it, the semaphore will still be 0,but there will be one fewer process sleeping on it ¶Implementation[3] ¶Uniprocessor solution 123456789101112131415161718192021222324252627282930class semaphore&#123; private int count; public semaphore (int init) &#123; count = init; &#125; public void P () &#123; while (1) &#123; Disable interrupts; if (count &gt; 0) &#123; count--; Enable interrupts; return; &#125; else &#123; Enable interrupts; &#125; &#125; &#125; public void V() &#123; Disable interrupts; count++; Enable interrupts; &#125;&#125; ¶Multiprocessor solution 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class semaphore&#123; private int flag; private int count; private queue q; public semaphore( int init ) &#123; flag = 0; count = init; q = new queue(); &#125; public void P() &#123; Disable interrupts; while ( TAS( flag ) != 0 ) &#123; /* just spin */ &#125;; if ( count &gt; 0 ) &#123; count--; flag = 0; Enable interrupts; return; &#125; Add process to q; flag = 0; Enable interrupts; Redispatch; &#125; public V() &#123; Disable interrupts; while ( TAS( flag ) != 0 ) &#123; /* just spin */ &#125;; if ( q == empty ) &#123; count++; &#125; else &#123; Remove first process from q and wake it up; &#125; flag = 0; Enable interrupts; &#125;&#125; The vendor of multi-core cpus has to take care that the different cores coordinate themselves when executing instructions which guarantee atomic memory access.[4] ¶Notes UNIVERSITY OF WISCONSIN-MADISON Computer Sciences Department Department of Computer Science, Kent State University Operating System Study Guide Threads ↩︎ pthread(7) ↩︎ Semaphore Implementation ↩︎ Critical sections with multicore processors ↩︎]]></content>
      <tags>
        <tag>note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Data Compression Note]]></title>
    <url>%2F2018%2F02%2F01%2FData_Compression%2F</url>
    <content type="text"><![CDATA[¶Data Compression ¶Huffman Table ¶Concept sort by frequency merge two smallest node until root node left ¶Example ¶Adaptive Huffman coding ¶Concept Each time the tree is updated either by adding a new character or increment the frequency of occurrence of an existing character If there is a node out of order, the structure of the tree is modified by exchanging the position of this node with the other node in the tree ¶Example the following example is transmitting a stream This-is [] is empty init index character output list in order? 1 T 0&quot;T&quot; []T1 Y index character output list in order? 2 h 0&quot;h&quot; [] h1 1 T1 Y index character output list in order? 3 i 00&quot;i&quot; [] i1 1 h1 2 T1 N index character output list in order? 3’ [] i1 1 h1 T1 2 Y index character output list in order? 4 s 100&quot;s&quot; [] s1 1 i1 2 h1 T1 3 N find the rightmost node which smaller than 2 to swap index character output list in order? 4’ [] s1 1 i1 T1 h1 2 2 Y index character output list in order? 5 - 000&quot;-&quot; [] -1 1 s1 2 i1 T1 h1 3 2 N index character output list in order? 5’ [] -1 1 s1 h1 i1 T1 2 2 3 Y index character output list in order? 6 i 01 [] -1 1 s1 h1 i2 T1 2 3 3 N index character output list in order? 6’ [] -1 1 s1 h1 T1 i2 2 2 4 Y index character output list in order? 7 s 111 [] -1 1 s2 h1 T1 i2 3 2 5 N index character output list in order? 7’ [] -1 1 T1 h1 s2 i2 2 3 4 Y ¶Arithmetic coding ¶Concept Represent a sequence of symbols by an interval with length equal to its probability The interval is specified by its lower boundary (l), upper boundary (u) and length d (=probability) The codeword for the sequence is the common bits in binary representations of l and u A more likely sequence=a longer interval=fewer bits each symbol (n with probability pn) is located in a interval among (0,1) qn-1 is the sum of probability of previous symbols ¶Encoding Process initial values l0=0 u0=1 d0=1 For nth symbol with qn-1 and pn at ln-1,un-1,dn-1=un-1-ln-1 dn = dn-1 * pn ln = ln-1 + dn-1 * qn-1 un = ln + dn ¶Example pa=1/2 pb=1/4 pc=1/4 qa=0 qb=pa=1/2 qc=pa+pb=3/4 For symbol stream abaca : symbol q p l u d 0 1 1 a 0 1/2 0=0+1x0 1/2=0+1/2 1/2=1x1/2 b 1/2 1/4 1/4=0+1/2x1/2 3/8=1/4+1/8 1/8=1/2x1/4 a 0 1/2 1/4=1/4+1/8x0 5/16=1/4+1/16 1/16=1/8x1/2 c 3/4 1/4 19/64=1/4+1/16x3/4 20/64=19/64+1/64 1/64=1/16x1/4 a 0 1/2 19/64=19/64+1/64x0 39/128=19/64+1/128 1/128=1/64x1/2 Output = value of u =39/128=0.0100111 (binary) ¶Decoding Process with input u and symbol interval in (0,1) 1.Push u into (0,1), and check which interval it is located, output the symbol own this interval. 2.u = u – low boundary of this interval 3.u = u / the range of this interval, ignore the carry 4.repeat step 1~3 till u is 0 ¶Example u is 0.0100111, with interval a(0,1/2], b(1/2,3/4], c(3/4,1] u interval 1/range symbol new u 39/128 (0,1/2] 2 a 39/64 39/64 (1/2,3/4] 4 b 7/16 7/16 (0,1/2] 2 a 7/8 7/8 (3/4,1] 4 c 1/2 1/2 (0,1/2] 2 a 0 Output:abaca ¶LZW ¶Property input:variable length output:fixed length dictionary based compression winzip winarj Using dictionary table that map character ( word, or character stream) to index (fixed bits) index New stream 0 prefix+character 1 prefix+character . . . . n prefix+character length of index, which dependent the size of dictionary, is the output of encoding ¶Encoding Process Define the size of DIC (dictionary) with 12 or 16 bits. Here is 12bits example Initialize the dictionary with the first 128 ASCII code, 000~07f (1)Set NS (new stream) as empty (2)Input NC (new character) check if NS+NC is in dictionary yes, NS=NS+NC, repeat (2) No, output the index of NS in dictionary append NS+NC into dictionary as new word NS=NC, repeat (2) till end of file (input) ¶Example A dictionary with root characters ABCD, encode ABCABCDABCABCAD Initial dictionary with 12 bits Dictionary &lt;000&gt; = “?” … &lt;041&gt; = “A” &lt;042&gt; = “B” &lt;043&gt; = “C” &lt;044&gt; = “D” … &lt;07f&gt; = “?” Input (NC) dictionary new stream output [ ] A ASCII code [A] B AB&gt;&gt;080 [B] 041, A C BC&gt;&gt;081 [C] 042, B A CA&gt;&gt;082 [A] 043, C B [AB] C ABC&gt;&gt;083 [C] 080, AB D CD&gt;&gt;084 [D] 043, C A DA&gt;&gt;085 [A] 044, D B [AB] C [ABC] A ABCA&gt;&gt;086 [A] 083, ABC B [AB] C [ABC] A [ABCA] D ABCAD&gt;&gt;087 [D] 086, ABCA End of file 044, D ¶Decoding Process NC: new code, NW: new word in dictionary Input first NC, NW=NC, output character of NC Input NC Check NC, if NC is in dictionary? Yes, set NW=NW+ first character in NC No. set NW=NW+ first character in NW Put NW into dictionary as new word, NW=NC Output the character stream of NC Is not EOF?, yes, repeat 1;otherwise, end decoding ¶Example Input code New word dictionary Output stream ASCII code 041 A 042 041+042 AB&gt;&gt;080 B 043 042+043 BC&gt;&gt;081 C 080 043+080 CA&gt;&gt;082 AB 043 080+043 ABC&gt;&gt;083 C 044 043+044 CD&gt;&gt;084 D 083 044+083 DA&gt;&gt;085 ABC 086 083+086 ABCA&gt;&gt;086 ABCA 044 086+044 ABCAD&gt;&gt;087 D End of file ¶Sound ¶Waveform coding Sample and code High-quality and not complex Large amount of bandwidth ¶PCM(Pulse-code modulation) A method used to digitally represent sampled analog signals. It is the standard form of digital audio in computers, compact discs, digital telephony and other digital audio applications Not really a compression technique ¶DPCM(Differential PCM) Only transmit the difference between the predicated value and the actual value It is possible to predict the value of a sample base on the values of previous samples The receiver perform the same prediction No algorithmic delay EX: Voice changes relatively slowly Take PCM sequence 250 268 269 241 for example, by using DPCM to improve, we can store the sequence into 250 +18 +19 -9 to save more space but doesn’t change the data, so DPCM is Lossless Compression. ¶ADPCM(Adaptive Differential PCM) Quantization level varies with local signal level locally estimated standard deviation From the example 250 +18 +19 -9 above,by using ADPCM, the sequence can be simplified as 250 (+9,+9,-4)x2 to save more space.But when decoding:(+9,+9,-4)x2=250, +18 +18 -8,the data is different from original source, so DPCM is Lossy Compression. ¶Source coding Based on a model of how the human voice is produced The waveform is discarded and parameters describing the characteristics of the sounds produced are transmitted instead. (Linear Predictive Coding) The synthesised voice tends to be rather artificial Generic models are used to reproduce vowels (vibrations of the vocal chords) and consonants (produced by shaping the mouth). Vocal Tract Model(不用記) G(z)=11−∑k=1PakZ−kG(z)={1 \over 1-\sum_{k=1}^Pa_kZ^{-k}} G(z)=​1−∑​k=1​P​​a​k​​Z​−k​​​​1​​ ak : LPC coefficients ¶LPC(Linear Prediction) Linear prediction is at the base of many speech coding techniques, including CELP. The idea behind it is to predict the signal using a linear combination of its past samples ¶CELP(Code-excited linear prediction)[1][2][3] Vocal cords are the source of spectrally flat sound (the excitation signal) Vocal tract acts as a filter to spectrally shape the excitation signal into various sounds. Use LPC to model the vocal tract Use Pitch Prediction: during voiced segments, the speech signal is periodic, so it is possible to take advantage of that property by approximating the excitation signal by a gain times the past of the excitation Use adaptive and fixed codebook entries as input (excitation) of the LP model The search performed in closed-loop in a “perceptually weighted domain” Analysis-by-Synthesis The encoding (analysis) is performed by perceptually optimising the decoded (synthesis) signal in a closed loop. In theory, the best CELP stream would be produced by trying all possible bit combinations and selecting the one that produces the best-sounding decoded signal. ¶MP3 ¶Absolute threshold of hearing is the minimum sound level of a pure tone that an average human ear with normal hearing can hear with no other sound present ¶Simultaneous masking occurs when a sound is made inaudible by a noise or unwanted sound of the same duration as the original sound.For example, a powerful spike at 1 kHz will tend to mask out a lower-level tone at 1.1 kHz. ¶Temporal Masking occurs when a sudden stimulus sound makes inaudible other sounds which are present immediately preceding or following the stimulus MP3 is based on the strength of the audio signal to achieve masking effect, thereby reducing the signal redundancy, to achieve the effect of compression ¶Run-length coding lossless data compression runs of data (that is, sequences in which the same data value occurs in many consecutive data elements) are stored as a single data value and count most useful on data that contains many such runs, and vice versa ¶Example WWWWWWWWWWWWBWWWWWWWWWWWWBBBWWWWWWWWWWWWWWWWWWWWWWWWBWWWWWWWWWWWWWW encode to 12W1B12W3B24W1B14W When using for compressing binary image, it can be combined with huffman table ¶READ coding Relative Element Address Designate coding Code the location of run boundary relative to the previous row. ¶Pass mode Condition: (b 1 b 2 ) is to the left of (a 1 a 2 ) and b 2 is strictly to the left of a 1 Run encoded: (b 1 b 2 ) Codeword §: 0001 + coded length of (b 1 b 2 ) Update: a 0 = b 2 &amp; the rest according to definitions ¶Horizontal mode Condition: (b 1 b 2 ) overlaps (a 1 a 2 ) by more than 3 pels as determined by |a 1 -b 1 | Runs encoded: (a 0 a 1 ), (a 1 a 2 ) Codeword (H): 001 + coded lengths of (a 0 a 1 ), (a 1 a 2 ) Update: a 0 = a 2 &amp; the rest according to definitions ¶Vertical mode Condition: (b 1 b 2 ) overlaps (a 1 a 2 ) by no more than 3 pels Expected as most common (assuming small differences) Run encoded: (a 1 b 1 ) Codeword: -3/-2/-1/0/1/2/3 → VR(3)/VR(2)/VR(1)/V(0)/VL(1)/VL(2)/VL(3) 1/000001/00001/011/1/010/000010/0000010 Update: a 0 = a 1 &amp; the rest according to definitions ¶JPEG Transform RGB to YUV, U and V down-sample 2x2. Blocking YUV into 8x8 blocks.Encoding blocks from right to left , top to down, Y, U and V in every block DCT transform into frequency domain. Quantize the DCT coefficients with default quantization table. Different weighting matrices are standardized, adapted to human visual contrast sensitivity. DC coefficient is coded with DPCM using previous block’s DC as predictor, then quantized. Run length coding as run length symbol with zig-zag order from DC to maximal freqency. EOB is the last symbol. Run length symbol (R,A) = ( zero count, non-zero value ). Entropy coding the run length symbols Use Huffman coding to compress data from entropy coding ¶JPEG2000 ¶Why need JPEG2000 For the users with different request in image quality or transmit bandwidth. In JPEG, many versions are coded and transmitted. For the users that are interested only one part in an image. (range of interest ROI) For the JPEG file with one part error. Scalable from lossless to high compressed in one compressed file. In order to address areas that the current standards fail to produce the quality or performance, as for example: Low bit-rate compression: For example below 0.25bpp Lossless and lossy compression: No current standard exists that can provide superiror lossy and lossless compression in a single codestream. Computer generated imagery: JPEG was optimized for natural imagery and does not perform well on computer generated imagery. Transmission in noisy environments: The current JPEG standard has provision for restart intervals, but image quality suffers dramatically when bit errors are encountered. Compound documents: Currentyly, JPEG is seldom used in the compression of compound documents because of its poor perfornamce when applied to bi-level(text) imagery. Open architecture Progressive transmission by pixel accuracy and resolution ¶Difference between JPEG New fuctionalities Region Of Interest Error resilience Progression orders Lossy to lossless in one system Better compression at low bit-rates Better at compound images and graphics ¶Codec ¶Image Coding System ¶MPEG Audio/video on CD-ROM Prompted explosion of digital video applications: MPEG1 video CD and downloadable video over Internet MPEG-1 Audio Offers 3 coding options (3 layers), higher layer have higher coding efficiency with more computations MP3 = MPEG1 layer 3 audio ¶Frame I-frames No temporal redundacy reduction Has the highest bit count For random access, FF, REW features P-frames Forward motion-compensated prediction B-frames Both forward and backward motion-compensated prediction Usually results in the lowest bit count Increase delay ¶Prediction Mode In P frame, the macro-block (2x2 blocks) is estimated that may move from M-block in previous frame (I or Previous P), the displacement of M-blocks between previous one and current one is the Motion Vector (MVx, MVy), the current M-B will be replaced (predicted) by the previous one, and the difference will be coded with JPEG ( Motion Compensation). ¶Bi-direction Mode In B frame, the M-B will be forward estimated from previous I or P frame with forward motion vector (MVfx,MVfy), and backward estimated from the following P frame with the backward motion vector (MVbx,MVby). The pixel value in B frame will be predicted by the respected pixels in previous frame and following frame with interpolation method. ¶Motion Estimation To estimate the macro-block in current frame ( P frame ) is moved from a macro-block on the previous frame ( I or P frame ) in the search area ( searching window ). Searching window : the area cover ± 7 (± 15) pixels centroid on the place of current macro-block. Motion Estimation : to find the best match M-block which embedded minimal MSE with the current M-block. Motion Vector : the distance between best match M-blocks and current M-blocks. MV( ± 7, ± 7 ) or MV( ± 15, ± 15 ). Motion Compensation : the difference between the current M-block and the best match M-block are coded by JPEG for compensating in the decoding process. ¶MPEG-2[4] A/V broadcast (TV, HDTV, Terrestrial, Cable, Satellite, High Speed Inter/Intranet) as well as DVD video MPEG-2 Audio Support 5.1 channel MPEG2 AAC: requires 30% fewer bits than MPEG1 layer 3 Different DCT modes and scanning methods are developed for interlaced sequences. Data partition All headers, MVs, first few DCT coefficients in the base layer Can be implemented at the bit stream level Simple SNR scalability Base layer includes coarsely quantized DCT coefficients Enhancement layer further quantizes the base layer quantization error Temporal scalability Spatial scalability MPEG-2 encodes video into double layer:Base layer and Enhancement layer. Base layer has higher transmission priority than enhancement layer, when receiver receive two layer’s data, base layer can be encoded individually to provide basic video quality.Enhancement layer provides some important data to combine with base layer to provide better video quality, so when some loss occurs to enhancement layer will not affect the video quality too much. ¶MPEG-4 Functionalities beyond MPEG-1/2 Interaction with individual objects The displayed scene can be composed by the receiver from coded objects Scalability of contents Error resilience Coding of both natural and synthetic audio and video ¶Object-Based Coding Entire scene is decomposed into multiple objects Each object is specified by its shape, motion, and texture (color) Shape and texture both changes in time (specified by motion) MPEG-4 assumes the encoder has a segmentation map available, specifies how to code (actually decode!) shape, motion and texture ¶H264 ¶Applications It is aimed at very low bit rate, real-time low end-to-end delay, and mobile applications such as conversational services and Internet video Enhanced visual quality at very low bit rates andparticularly at rate below 24kb/s ¶Common Technical Elements with other Standards 16x16 macroblocks Conventional 4:2:0 sampling of chrominance and association of luminance and chrominance data (note 4:2:2 and 4:4:4 being planned for future revision) Block motion displacement Motion vectors over picture boundaries Variable block-size motion compensation Block transforms Scalar quantization I, P and B picture types ¶New Features of H.264 Variable block-sized motion compensation with small block size Multi-mode, multi-reference motion compensated Motion vector can point out of image border 1/4-, 1/8-pixel motion vector precision B-frame prediction weighting 4x4 integer DCT transform Multi-mode intra-prediction In-loop de-blocking filter UVLC (Uniform Variable Length Coding) SP-slices ¶Channel coding ¶Linear Block code Assume the original data has k bits, plus n-k bits of parity check bits, the Code Rate R = k/n, when the data length and parity check bits length have linear relationship, it is called linear block code. ¶cyclic code 指說一段長度為n bits的編碼，本身屬於finite field(GF field)，經過循環位移(如圖)後，仍然屬於GF field，這樣的結構性質可以運用在錯誤控制編碼。 ¶RS code 透過GF field來進行，以GF(8)為例p(x)=x^3+x+1 本質元素 滿足α^3+α+1=0 and a^7=1 GF(8)總共有8個元素，可以以指數和多項式型態來表示，運用於錯誤控制碼 過程太複雜，在此不敘述 ¶CIRC(cross-interleaved Reed–Solomon code) coding Codecs used by CD drives, including two Reed–Solomon Codes, (32,28) and (28,24) respectively, each symbol 8 bytes, the first one (32,28), so redundancy is 4 bits, which can correct two error, and so does (28,24), and use special technique called cross interleaving to combine them。32 bits And there is total (28/32)x(24/28)=75% contains data. The advantage is that there isn’t too many errors it can be corrected directly.If there is too many errors it can detect and create repaired audio by interpolation method. With errror control code, if there are small scratches, dust and fingerprints on CD and still in range of repair ability, then the audio sounds no differene. ¶Watermarking Robustness：A digital watermark is called robust if it resists a designated class of transformations. Robust watermarks may be used in copy protection applications to carry copy and no access control information. Perceptibility：A digital watermark is called imperceptible if the original cover signal and the marked signal are perceptually indistinguishable ¶Refs [5] [6] Introduction to CELP Coding ↩︎ Codetopic-excited linear predictive (CELP) coders (VoIP Protocols) ↩︎ CELP語音編碼器框架遺失訊號重整之研究 ↩︎ MPEG-2原理 ↩︎ Magnitude, real and phase images ↩︎ FFT of image data: “mirroring” to avoid boundary effects ↩︎]]></content>
      <tags>
        <tag>note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Compact Virtual Machine (CVM)]]></title>
    <url>%2F2018%2F02%2F01%2Fcvm%2F</url>
    <content type="text"><![CDATA[¶Compact Virtual Machine (CVM) ¶Introduction The Java 2 Platform, Micro Edition (J2ME) is aimed at the market for consumer and embedded electronic devices: cellular telephones, two-way pagers, Personal Digital Assistants (PDAs), set-top boxes, and other small devices. Connected Limited Device Configuration(CLDC):cellular phones, pagers, PDAs Connected Device Configuration(CDC):set-top boxes, and Internet TVs Mobile Information Device Profile (MIDP) runs on top of the CLDC which includes the KVM virtual machine. Several profiles run on top of CDC such as foundation, Personal andPersonal Basis. In this article will focus on CDC’s JVM:Compact Virtual Machine(CVM). CVM’s capabilities are similar to the JVMs that support the Java 2 Platform, Standard Edition (J2SE). The main differences lie in the memory and display capabilities of the devices targeted. Is powered by a 32-bit processor Makes available to the Java runtime at least 2 MB of memory, including both RAM and flash memory or ROM Requires the full functionality of the Java Virtual Machine Specification Has connectivity to some kind of network, often wireless May have a user interface with some degree of sophistication ¶Porting To order to fit my research’s need, I made some modifications. Ignore java class file version check Compile with 32-bit mode in x86_64 system repo ¶Build ¶x86_32 Makefile is in cdc/build/linux-x86-generic On 64-bit system, only 32-bit mode is allowed, so I modified ASM_ARCH_FLAGS,CC_ARCH_FLAGSandLINK_ARCH_FLAGS by adding -m32 option which already committed in repo above Build with 1make CVM_TARGET_TOOLS_PREFIX=/usr/bin/ J2ME_CLASSLIB=foundation ¶ARM There is an issue needed to concern when running CVM on ARM devices. Build with 1make CVM_TARGET_TOOLS_PREFIX=/usr/bin/arm-linux-gnueabi- J2ME_CLASSLIB=foundation USE_AAPCS=true CC_ARCH_FLAGS='-marm' ¶Run Test 1bin/cvm -cp testclasses.zip Test ¶Reference Liunx for zedboard resources build by my self J2ME for Home Appliances and Consumer Electronic Devices CDC Build System Guide CDC Porting Guide CDC Runtime Guide How to Port phoneME Advanced Software JAVA ON HANDHELD DEVICES - COMPARING J2ME CDC TO JAVA 1.1 AND JAVA 2 CVM Stacks and Code Execution]]></content>
      <tags>
        <tag>NCTU</tag>
      </tags>
  </entry>
</search>
